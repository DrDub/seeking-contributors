{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = False\n",
    "if beta:\n",
    "    # Implementation from https://dev.to/davidisrawi/build-a-quick-summarizer-with-python-and-nltk\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "\n",
    "    def _create_frequency_table(text_string) -> dict:\n",
    "        \"\"\"\n",
    "        we create a dictionary for the word frequency table.\n",
    "        For this, we should only use the words that are not part of the stopWords array.\n",
    "        Removing stop words and making frequency table\n",
    "        Stemmer - an algorithm to bring words to its root word.\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        stopWords = set(stopwords.words(\"english\"))\n",
    "        words = word_tokenize(text_string)\n",
    "        ps = PorterStemmer()\n",
    "\n",
    "        freqTable = dict()\n",
    "        for word in words:\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "            if word in freqTable:\n",
    "                freqTable[word] += 1\n",
    "            else:\n",
    "                freqTable[word] = 1\n",
    "\n",
    "        return freqTable\n",
    "\n",
    "\n",
    "    def _score_sentences(sentences, freqTable) -> dict:\n",
    "        \"\"\"\n",
    "        score a sentence by its words\n",
    "        Basic algorithm: adding the frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "\n",
    "        sentenceValue = dict()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            word_count_in_sentence = (len(word_tokenize(sentence)))\n",
    "            word_count_in_sentence_except_stop_words = 0\n",
    "            for wordValue in freqTable:\n",
    "                if wordValue in sentence.lower():\n",
    "                    word_count_in_sentence_except_stop_words += 1\n",
    "                    if sentence[:10] in sentenceValue:\n",
    "                        sentenceValue[sentence[:10]] += freqTable[wordValue]\n",
    "                    else:\n",
    "                        sentenceValue[sentence[:10]] = freqTable[wordValue]\n",
    "\n",
    "            if sentence[:10] in sentenceValue:\n",
    "                sentenceValue[sentence[:10]] = sentenceValue[sentence[:10]] / word_count_in_sentence_except_stop_words\n",
    "\n",
    "            '''\n",
    "            Notice that a potential issue with our score algorithm is that long sentences will have an advantage over short sentences.\n",
    "            To solve this, we're dividing every sentence score by the number of words in the sentence.\n",
    "\n",
    "            Note that here sentence[:10] is the first 10 character of any sentence, this is to save memory while saving keys of\n",
    "            the dictionary.\n",
    "            '''\n",
    "\n",
    "        return sentenceValue\n",
    "\n",
    "\n",
    "    def _find_average_score(sentenceValue) -> int:\n",
    "        \"\"\"\n",
    "        Find the average score from the sentence value dictionary\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        sumValues = 0\n",
    "        for entry in sentenceValue:\n",
    "            sumValues += sentenceValue[entry]\n",
    "\n",
    "        # Average value of a sentence from original text\n",
    "        average = (sumValues / len(sentenceValue))\n",
    "\n",
    "        return average\n",
    "\n",
    "\n",
    "    def _generate_summary(sentences, sentenceValue, threshold):\n",
    "        sentence_count = 0\n",
    "        summary = ''\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if sentence[:10] in sentenceValue and sentenceValue[sentence[:10]] >= (threshold):\n",
    "                summary += \" \" + sentence\n",
    "                sentence_count += 1\n",
    "\n",
    "        return summary\n",
    "\n",
    "\n",
    "    def run_summarization(text):\n",
    "        # 1 Create the word frequency table\n",
    "        freq_table = _create_frequency_table(text)\n",
    "\n",
    "        '''\n",
    "        We already have a sentence tokenizer, so we just need\n",
    "        to run the sent_tokenize() method to create the array of sentences.\n",
    "        '''\n",
    "\n",
    "        # 2 Tokenize the sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "\n",
    "        # 3 Important Algorithm: score the sentences\n",
    "        sentence_scores = _score_sentences(sentences, freq_table)\n",
    "\n",
    "        # 4 Find the threshold\n",
    "        threshold = _find_average_score(sentence_scores)\n",
    "\n",
    "        # 5 Important Algorithm: Generate the summary\n",
    "        summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
    "\n",
    "        return summary\n",
    "\n",
    "    text_str = ' nothing is more important than the health of your family. ' \n",
    "    if __name__ == '__main__':\n",
    "        result = run_summarization(text_str)\n",
    "        print(result)\n",
    "\n",
    "\n",
    "    # credit: https://github.com/akashp1712/nlp-akash/blob/master/text-summarization/Word_Frequency_Summarization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://github.com/MechanicalSoup/MechanicalSoup\"><img width=\"278\" src=\"https://denvercoder1-github-readme-stats.vercel.app/api/pin/?username=MechanicalSoup&repo=MechanicalSoup&theme=react&bg_color=1F222E&title_color=9ACD32&hide_border=true&icon_color=FF4500&show_icons=false\" alt=\"MechanicalSoup\"></a>\n",
      "\n",
      "Paste the block below into the the bottom row of the table at the end of the readme.\n",
      "\n",
      "| [MechanicalSoup](https://github.com/MechanicalSoup/MechanicalSoup) |  ![last commit](https://img.shields.io/github/last-commit/MechanicalSoup/MechanicalSoup) ![code size](https://img.shields.io/github/languages/code-size/MechanicalSoup/MechanicalSoup) ![commit activity](https://img.shields.io/github/commit-activity/m/MechanicalSoup/MechanicalSoup) ![issues](https://img.shields.io/github/issues/MechanicalSoup/MechanicalSoup) | A Python library for automating interaction with websites. MechanicalSoup automatically stores and sends cookies, follows redirects, and can follow links and submit forms. It doesn't do JavaScript.  MechanicalSoup was created by M Hickford, who was a fond user of the Mechanize library. Unfortunately, Mechanize was incompatible with Python 3 until 2019 and its development stalled for several years. MechanicalSoup provides a similar API, built on Python giants Requests (for HTTP sessions) and BeautifulSoup (for document navigation). Since 2017 it is a project actively maintained by a small team including @hemberger and @moy. |\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#username = '{}'\n",
    "#project_name = 'https://github.com/{}/PySeas'\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_text_from_url(url):\n",
    "    # add #readme to url to get the readme file\n",
    "    url = url + '#readme'\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'lxml') # lxml is the parser\n",
    "    # kill all script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()    # rip it out\n",
    "    # get text\n",
    "    text = soup.get_text()\n",
    "    # break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    return text\n",
    "\n",
    "\n",
    "def create_project_tile(project_url):\n",
    "\n",
    "    \"\"\"\n",
    "    create_project_tile creates a markdown ready tile for the project.\n",
    "    example url: https://github.com/{}/seeking-contributors\n",
    "    :param username: your github user name as it appears in the url of your profile (i.e. '{}' in the url above).\n",
    "    :type username: str\n",
    "    :param project_name: the exact name of the project as it appears in the url of it's repo main page. In the example above it would be 'seeking-contributors'\n",
    "    :type project_name: str\n",
    "    :return: the markdown text for your project tile.\n",
    "    :rtype: str\n",
    "    \"\"\"\n",
    "    project_name = project_url.split('/')[-1] # negative index to get last\n",
    "    username = project_url.split('/')[-2] # get the username that came before the second to last /\n",
    "    markdown_string = \"\"\"<a href=\"https://github.com/{}/{}\"><img width=\"278\" src=\"https://denvercoder1-github-readme-stats.vercel.app/api/pin/?username={}&repo={}&theme=react&bg_color=1F222E&title_color=9ACD32&hide_border=true&icon_color=FF4500&show_icons=false\" alt=\"{}\"></a>\"\"\".format(username,project_name,username,project_name,project_name)\n",
    "    if beta:\n",
    "        # summarize the project by getting the readme file and reading just the first paragraph of it and summarizing it.\n",
    "        # step 1 get the text from the readme file using requests and beautiful soup (bs4) libraries and finding the first paragraph.\n",
    "        text = get_text_from_url(project_url)\n",
    "        # the first paragraph is the first line of the text\n",
    "        first_paragraph = text.split('\\n')[0] # split the text by new line and get the first element\n",
    "        # if the first paragraph is less than 100 characters, then it's probably not a good summary, so move on to the next paragraph.\n",
    "        if len(first_paragraph) < 100:\n",
    "            first_paragraph = text.split('\\n')[1]\n",
    "        if len(first_paragraph) < 100:\n",
    "            first_paragraph = text.split('\\n')[2]\n",
    "        if len(first_paragraph) < 100:\n",
    "            first_paragraph = text.split('\\n')[3]\n",
    "        \n",
    "        print(first_paragraph)\n",
    "        # step 2 summarize the text using the summarize function from the gensim library\n",
    "        summary = run_summarization(first_paragraph)\n",
    "        # step 3 add the summary to the markdown string\n",
    "        #markdown_string += \"\"\"<p align=\"center\">{}</p>\"\"\".format(summary)\n",
    "    else:\n",
    "        summary = input(\"Enter a short description of the project: \")\n",
    "    table_row  = \"\"\"| [{}](https://github.com/{}/{}) |  ![last commit](https://img.shields.io/github/last-commit/{}/{}) ![code size](https://img.shields.io/github/languages/code-size/{}/{}) ![commit activity](https://img.shields.io/github/commit-activity/m/{}/{}) ![issues](https://img.shields.io/github/issues/{}/{}) | {} |\"\"\".format(project_name,username,project_name,username,project_name,username,project_name,username,project_name,username,project_name,summary)\n",
    "    \n",
    "    # concatenate the two strings with four line breaks in between them\n",
    "    result =  markdown_string + '\\n\\n' + 'Paste the block below into the the bottom row of the table at the end of the readme.' + '\\n\\n' + table_row    \n",
    "    return result\n",
    "\n",
    "\n",
    "project_url = input(\"Enter the url of your project: \")\n",
    "\n",
    "markdown_string = create_project_tile(project_url)\n",
    "\n",
    "print(markdown_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://github.com/talkygram/webcrawly\"><img width=\"278\" src=\"https://denvercoder1-github-readme-stats.vercel.app/api/pin/?username=talkygram&repo=webcrawly&theme=react&bg_color=1F222E&title_color=9ACD32&hide_border=true&icon_color=FF4500&show_icons=false\" alt=\"webcrawly\"></a>\n",
      "\n",
      "Paste the block below into the the bottom row of the table at the end of the readme.\n",
      "\n",
      "| [webcrawly](https://github.com/talkygram/webcrawly) |  ![last commit](https://img.shields.io/github/last-commit/talkygram/webcrawly) ![code size](https://img.shields.io/github/languages/code-size/talkygram/webcrawly) ![commit activity](https://img.shields.io/github/commit-activity/m/talkygram/webcrawly) ![issues](https://img.shields.io/github/issues/talkygram/webcrawly) |  |\n",
      "<a href=\"https://github.com/BKAmos/DataScience\"><img width=\"278\" src=\"https://denvercoder1-github-readme-stats.vercel.app/api/pin/?username=BKAmos&repo=DataScience&theme=react&bg_color=1F222E&title_color=9ACD32&hide_border=true&icon_color=FF4500&show_icons=false\" alt=\"DataScience\"></a>\n",
      "\n",
      "Paste the block below into the the bottom row of the table at the end of the readme.\n",
      "\n",
      "| [DataScience](https://github.com/BKAmos/DataScience) |  ![last commit](https://img.shields.io/github/last-commit/BKAmos/DataScience) ![code size](https://img.shields.io/github/languages/code-size/BKAmos/DataScience) ![commit activity](https://img.shields.io/github/commit-activity/m/BKAmos/DataScience) ![issues](https://img.shields.io/github/issues/BKAmos/DataScience) |  |\n",
      "<a href=\"https://github.com/VanillaLattA/DatascienceShortcuts\"><img width=\"278\" src=\"https://denvercoder1-github-readme-stats.vercel.app/api/pin/?username=VanillaLattA&repo=DatascienceShortcuts&theme=react&bg_color=1F222E&title_color=9ACD32&hide_border=true&icon_color=FF4500&show_icons=false\" alt=\"DatascienceShortcuts\"></a>\n",
      "\n",
      "Paste the block below into the the bottom row of the table at the end of the readme.\n",
      "\n",
      "| [DatascienceShortcuts](https://github.com/VanillaLattA/DatascienceShortcuts) |  ![last commit](https://img.shields.io/github/last-commit/VanillaLattA/DatascienceShortcuts) ![code size](https://img.shields.io/github/languages/code-size/VanillaLattA/DatascienceShortcuts) ![commit activity](https://img.shields.io/github/commit-activity/m/VanillaLattA/DatascienceShortcuts) ![issues](https://img.shields.io/github/issues/VanillaLattA/DatascienceShortcuts) |  |\n"
     ]
    }
   ],
   "source": [
    "# urls = ['https://github.com/talkygram/webcrawly','https://github.com/BKAmos/DataScience','https://github.com/VanillaLattA/DatascienceShortcuts']\n",
    "\n",
    "# for url in urls:\n",
    "#     markdown_string = create_project_tile(url)\n",
    "#     print(markdown_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('master_env_temp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "666dfdc1c3434f60b2f64fdb13e0b185df8f5878ddae1a39c75e1eb2af091f89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
